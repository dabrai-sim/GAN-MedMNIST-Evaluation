{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Title: Evaluation of Generative Adversarial Networks (GANs) using Inception Score and Fréchet Inception Distance"
      ],
      "metadata": {
        "id": "BmkTi0v55M0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objective:\n",
        "- To evaluate the performance of a trained GAN using Inception Score (IS) and Fréchet Inception Distance (FID).\n",
        "\n",
        "- To visualize generated images and evaluation metrics using TensorBoard."
      ],
      "metadata": {
        "id": "oD8LQjdS5Tpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory:\n",
        "- Generative Adversarial Networks (GANs)\n",
        "GANs are deep learning models used for generating realistic synthetic data.\n",
        "\n",
        "- They consist of two neural networks: Generator (G) and Discriminator (D), trained in a min-max game.\n",
        "\n",
        "##Evaluation Metrics:\n",
        "- Inception Score (IS): Measures the quality and diversity of generated images. Higher scores indicate better quality.\n",
        "\n",
        "- Fréchet Inception Distance (FID): Measures the distance between real and generated images in feature space. Lower scores indicate more realistic images.\n",
        "\n",
        "##TensorBoard Visualization\n",
        "TensorBoard helps in logging images, graphs, and metrics for easy visualization."
      ],
      "metadata": {
        "id": "MkXiG6Gs5cfa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siHyi9fn5IEK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from medmnist import PathMNIST\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "from torchmetrics.image.inception import InceptionScore\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "import torch_fidelity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFQ2pU8M5IEK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crE6Fw7V5IEK"
      },
      "outputs": [],
      "source": [
        "from medmnist import PathMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7et4QY15IEL"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZP--Ftb5IEL"
      },
      "outputs": [],
      "source": [
        "dataset = PathMNIST(root=r\"C:\\Users\\Simrann\\Downloads\\qwerty\\gans\\assignment4\\root\", split=\"train\", transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtBSidre5IEM",
        "outputId": "88e2beab-43cf-45fa-80e6-ccc65bb7626e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset PathMNIST of size 28 (pathmnist)\n",
              "    Number of datapoints: 89996\n",
              "    Root location: C:\\Users\\Simrann\\Downloads\\qwerty\\gans\\assignment4\\root\n",
              "    Split: train\n",
              "    Task: multi-class\n",
              "    Number of channels: 3\n",
              "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
              "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
              "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
              "    License: CC BY 4.0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import Subset\n",
        "subset_indices = np.random.choice(len(dataset), 10000, replace=False)\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "dataloader = DataLoader(subset_dataset, batch_size=64, shuffle=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv-4Bn5B5IEM",
        "outputId": "ef9b4f4e-bd9a-4a08-8203-be0a74b69903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: torch.Size([64, 3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "for img, _ in dataloader:\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKvsiM_o5IEM"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 28 * 28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), 1, 28, 28)  # Original shape: (batch_size, 784)\n",
        "        img = img.repeat(1, 3, 1, 1)          # Repeat the single channel 3 times\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfKSFGiO5IEM"
      },
      "outputs": [],
      "source": [
        "#Discriminator for LS-GAN\n",
        "# Fix for RGB images in MedMNIST (28x28x3 = 2352)\n",
        "class DiscriminatorLS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorLS, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28 * 3, 512),  # Change 784 → 28*28*3 = 2352\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img.view(img.size(0), -1))  # Flatten properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_pbrLLL5IEN"
      },
      "outputs": [],
      "source": [
        "# Define WGAN and WGAN-GP Discriminator\n",
        "class DiscriminatorW(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorW, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28 * 3, 512),  # Change 784 → 28*28*3 = 2352\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img.view(img.size(0), -1))  # Flatten properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMX9mj-d5IEN"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_gan(generator, discriminator, g_optimizer, d_optimizer, loss_fn, epochs, gan_type):\n",
        "    writer = SummaryWriter(log_dir=f'logs/{gan_type}')\n",
        "    for epoch in range(epochs):\n",
        "        for i, (real_imgs, _) in enumerate(dataloader):\n",
        "            real_imgs = real_imgs.to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            z = torch.randn(real_imgs.size(0), 100).to(device)\n",
        "            fake_imgs = generator(z).detach()\n",
        "            d_real = discriminator(real_imgs)\n",
        "            d_fake = discriminator(fake_imgs)\n",
        "\n",
        "            d_loss = 0\n",
        "\n",
        "            if gan_type == \"LS-GAN\":\n",
        "                d_loss = 0.5 * ((d_real - 1) ** 2).mean() + 0.5 * (d_fake ** 2).mean()\n",
        "            elif gan_type == \"WGAN\":\n",
        "                d_loss = -torch.mean(d_real) + torch.mean(d_fake)\n",
        "            elif gan_type == \"WGAN-GP\":\n",
        "                lambda_gp = 10\n",
        "                epsilon = torch.rand(real_imgs.size(0), 1, 1, 1).to(device)\n",
        "                x_hat = (epsilon * real_imgs + (1 - epsilon) * fake_imgs).requires_grad_(True)\n",
        "                d_x_hat = discriminator(x_hat)\n",
        "                gradients = torch.autograd.grad(outputs=d_x_hat, inputs=x_hat,\n",
        "                                                grad_outputs=torch.ones_like(d_x_hat),\n",
        "                                                create_graph=True, retain_graph=True)[0]\n",
        "                gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "                d_loss += lambda_gp * gp\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            z = torch.randn(real_imgs.size(0), 100).to(device)\n",
        "            fake_imgs = generator(z)\n",
        "            g_fake = discriminator(fake_imgs)\n",
        "\n",
        "            if gan_type == \"LS-GAN\":\n",
        "                g_loss = 0.5 * ((g_fake - 1) ** 2).mean()\n",
        "            else:\n",
        "                g_loss = -torch.mean(g_fake)\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "        # Save progress\n",
        "        if epoch % 10 == 0:\n",
        "            vutils.save_image(fake_imgs, f\"generated_{gan_type}_{epoch}.png\", normalize=True)\n",
        "            writer.add_scalar(f'{gan_type}/D_Loss', d_loss.item(), epoch)\n",
        "            writer.add_scalar(f'{gan_type}/G_Loss', g_loss.item(), epoch)\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N-GrUne5IEN"
      },
      "outputs": [],
      "source": [
        "ls_generator = Generator(100).to(device)\n",
        "ls_discriminator = DiscriminatorLS().to(device)\n",
        "g_optimizer_ls = optim.Adam(ls_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d_optimizer_ls = optim.Adam(ls_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeeDId0F5IEN"
      },
      "outputs": [],
      "source": [
        "generator_w = Generator(100).to(device)\n",
        "discriminator_w = DiscriminatorW().to(device)\n",
        "g_optimizer_w = optim.RMSprop(generator_w.parameters(), lr=0.00005)\n",
        "d_optimizer_w = optim.RMSprop(discriminator_w.parameters(), lr=0.00005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkUPqCVW5IEO"
      },
      "outputs": [],
      "source": [
        "generator_gp = Generator(100).to(device)\n",
        "discriminator_gp = DiscriminatorW().to(device)\n",
        "g_optimizer_gp = optim.Adam(generator_gp.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d_optimizer_gp = optim.Adam(discriminator_gp.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBRnYG365IEO"
      },
      "outputs": [],
      "source": [
        "train_gan(ls_generator, ls_discriminator, g_optimizer_ls, d_optimizer_ls, nn.MSELoss(), 50, \"LS-GAN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_gM2bSR5IEO"
      },
      "outputs": [],
      "source": [
        "train_gan(generator_w, discriminator_w, g_optimizer_w, d_optimizer_w, None, 50, \"WGAN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUxFJYHD5IEO"
      },
      "outputs": [],
      "source": [
        "train_gan(generator_gp, discriminator_gp, g_optimizer_gp, d_optimizer_gp, None, 50, \"WGAN-GP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDNd9M95IEO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.image.inception import InceptionScore\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def evaluate_gan(generator, num_images=100, latent_dim=100, device='cuda', log_dir=\"logs\"):\n",
        "    generator.eval()\n",
        "    generator.to(device)\n",
        "\n",
        "    writer = SummaryWriter(log_dir)  # Initialize TensorBoard writer\n",
        "    fake_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_images):\n",
        "            z = torch.randn(1, latent_dim, device=device)\n",
        "            generated = generator(z)\n",
        "\n",
        "            if generated.dim() == 3:\n",
        "                generated = generated.unsqueeze(0)\n",
        "            elif generated.dim() == 2:\n",
        "                generated = generated.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            if generated.shape[1] == 1:\n",
        "                generated = generated.expand(-1, 3, -1, -1)\n",
        "\n",
        "            fake_images.append(generated.cpu())\n",
        "\n",
        "    fake_images = torch.cat(fake_images, dim=0)\n",
        "\n",
        "    # Normalize images for visualization\n",
        "    fake_images = (fake_images - fake_images.min()) / (fake_images.max() - fake_images.min())\n",
        "\n",
        "    # Convert to uint8 for metrics\n",
        "    fake_images_uint8 = (fake_images * 255).clamp(0, 255).byte()\n",
        "\n",
        "    # Log generated images to TensorBoard\n",
        "    img_grid = make_grid(fake_images[:25], nrow=5)  # Show first 25 images in a 5x5 grid\n",
        "    writer.add_image(\"Generated Images\", img_grid, 0)\n",
        "\n",
        "    # Compute Inception Score\n",
        "    is_metric = InceptionScore().to(device)\n",
        "    is_mean, is_std = is_metric(fake_images_uint8.to(device))\n",
        "    is_score = is_mean.item()\n",
        "    writer.add_scalar(\"Inception Score\", is_score, 0)\n",
        "\n",
        "    # Compute FID Score\n",
        "    fid_metric = FrechetInceptionDistance().to(device)\n",
        "    fid_metric.update(fake_images_uint8.to(device), real=False)\n",
        "    fid_metric.update(fake_images_uint8.to(device), real=True)  # Replace with real images\n",
        "    fid_score = fid_metric.compute().item()\n",
        "    writer.add_scalar(\"FID Score\", fid_score, 0)\n",
        "\n",
        "    writer.close()  # Close TensorBoard writer\n",
        "\n",
        "    print(f\"Inception Score: {is_score:.4f} ± {is_std.item():.4f}\")\n",
        "    print(f\"FID Score: {fid_score:.4f}\")\n",
        "\n",
        "    return is_score, fid_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-hwtSnM5IEO",
        "outputId": "5a81045a-9035-4cd0-97ae-bbb0051d4dd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Simrann\\anaconda3\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inception Score: 1.0008 ± 0.0003\n",
            "FID Score: -0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0008093118667603, -1.3069113720121095e-06)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_gan(ls_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCy0uuBC5IEO",
        "outputId": "385dae1a-b1e3-4545-e80a-20727dfab5a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inception Score: 1.0930 ± 0.0430\n",
            "FID Score: -0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.092960238456726, -3.7507892557187006e-05)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_gan(generator_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrEFClgE5IEO",
        "outputId": "1dadbe13-2cf3-4f84-9146-35efad3bf310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inception Score: 1.0000 ± 0.0000\n",
            "FID Score: -0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0, -1.1001999311588406e-10)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_gan(generator_gp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}